{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f2c2b",
   "metadata": {},
   "source": [
    "# Tutorial on ensemble history matching and optimisation\n",
    "\n",
    "Copyright Patrick N. Raanes, NORCE, 2020.\n",
    "\n",
    "This (Jupyter/Python) notebook is a self-contained tutorial on\n",
    "history matching (HM) using ensemble methods.\n",
    "Please do not hesitate to file issues on GitHub,\n",
    "or submit pull requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a5bec",
   "metadata": {},
   "source": [
    "## The Jupyter notebook format\n",
    "Notebooks combine **cells** of code (Python) with cells of text (markdown).\n",
    "For example, try to edit the cell below to insert your name, and then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Batman\"\n",
    "print(\"Hello world! I'm \" + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf8c4f",
   "metadata": {},
   "source": [
    "You will likely be more efficient if you know these **keyboard shortcuts**:\n",
    "\n",
    "| Navigate                      |    | Edit              |    | Exit           |    | Run                              |    | Run & advance                     |\n",
    "| -------------                 | -- | ----------------- | -- | --------       | -- | -------                          | -- | -------------                     |\n",
    "| <kbd>↓</kbd> and <kbd>↑</kbd> |    | <kbd>Enter</kbd>  |    | <kbd>Esc</kbd> |    | <kbd>Ctrl</kbd>+<kbd>Enter</kbd> |    | <kbd>Shift</kbd>+<kbd>Enter</kbd> |\n",
    "\n",
    "When you open a notebook it starts a **session (kernel/runtime)** of Python\n",
    "in the background.  All of the code cells (in a given notebook) are connected\n",
    "(they use the same kernel and thus share variables, functions, and classes).\n",
    "Thus, the **order** in which you run the cells matters.  One thing you must\n",
    "know is how to **restart** the session, so that you can start over. Try to\n",
    "locate this option via the top menu bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730a86c",
   "metadata": {},
   "source": [
    "There is a huge amount of libraries available in **Python**,\n",
    "including the popular `numpy` and `matplotlib/pyplot` packages.\n",
    "These are imported (and abbreviated) as `np`, and `mpl` and `plt`.\n",
    "Try them out by running the following, which illustrates some algebra\n",
    "using syntax reminiscent of Matlab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea62e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tools import mpl_setup\n",
    "mpl_setup.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a792ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy's arrays for vectors and matrices. Example constructions:\n",
    "a  = np.arange(10)  # Alternatively: np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "Id = 2*np.eye(10)   # Alternatively: np.diag(2*np.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indexing examples:\")\n",
    "print(\"a         =\", a)\n",
    "print(\"a[3]      =\", a[3])\n",
    "print(\"a[0:3]    =\", a[0:3])\n",
    "print(\"a[:3]     =\", a[:3])\n",
    "print(\"a[3:]     =\", a[3:])\n",
    "print(\"a[-1]     =\", a[-1])\n",
    "print(\"Id[:3,:3] =\", Id[:3, :3], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLinear algebra examples:\")\n",
    "print(\"100+a  =\", 100+a)\n",
    "print(\"Id@a   =\", Id@a)\n",
    "print(\"Id*a   =\", Id*a, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03421900",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Plotting example\")\n",
    "plt.ylabel(\"$i \\\\, x^2$\")\n",
    "for i in range(4):\n",
    "    plt.plot(i * a**2, label=\"i = %d\" % i)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c1164",
   "metadata": {},
   "source": [
    "Run the following cells to import yet more tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d5142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy.random as rnd\n",
    "import scipy.linalg as sla\n",
    "from matplotlib.ticker import LogLocator\n",
    "from mpl_tools.place import freshfig\n",
    "from numpy import sqrt\n",
    "from struct_tools import DotDict as Dict\n",
    "from tqdm.auto import tqdm as progbar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e1584",
   "metadata": {},
   "source": [
    "## Model and case specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e36d14",
   "metadata": {},
   "source": [
    "For exact reproducibility of our problem/case, we set the random generator seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = rnd.seed(4)  # very easy\n",
    "# seed = rnd.seed(5)  # hard\n",
    "# seed = rnd.seed(6)  # very easy\n",
    "# seed = rnd.seed(7)  # easy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d45f5e",
   "metadata": {},
   "source": [
    "Our reservoir simulator takes up about 100 lines of python code. This may seem\n",
    "outrageously simple, but serves the purpose of *illustrating* the main features of\n",
    "the history matching process. Indeed, we do not detail the simulator code here, but\n",
    "simply import it from the accompanying python modules, together with the associated\n",
    "plot functionality, the (geostatistical) random field generator, and some linear\n",
    "algebra. Hence our focus and code will be of aspects directly related to the history\n",
    "matching and optimisation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulator\n",
    "import simulator.plotting as plots\n",
    "from tools import geostat, misc\n",
    "from tools.misc import center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cc2a0",
   "metadata": {},
   "source": [
    "In short, the model is a 2D, two-phase, immiscible, incompressible simulator using\n",
    "two-point flux approximation (TPFA) discretisation. It was translated from the Matlab\n",
    "code here http://folk.ntnu.no/andreas/papers/ResSimMatlab.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simulator.ResSim(Nx=20, Ny=20, Lx=2, Ly=1)\n",
    "\n",
    "# Also init plots module\n",
    "plots.model = model\n",
    "plots.coord_type = \"absolute\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440a86e",
   "metadata": {},
   "source": [
    "The following declares some data containers to help us keep organised.\n",
    "The names have all been shortened to 4 characters, but this is just\n",
    "to obtain more convenient code alignment and readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e09c20",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Permeability\n",
    "perm = Dict()\n",
    "\n",
    "# Production\n",
    "prod = Dict(\n",
    "    past = Dict(),\n",
    "    futr = Dict(),\n",
    ")\n",
    "\n",
    "# Water saturation\n",
    "wsat = Dict(\n",
    "    init = Dict(),\n",
    "    past = Dict(),\n",
    "    futr = Dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80aee84",
   "metadata": {},
   "source": [
    "Technical note: This data hierarchy is convienient in *this* notebook/script,\n",
    "especially for plotting purposes. For example, we can with ease refer to\n",
    "`wsat.past.Truth` and `wsat.past.Prior`. The former will be a numpy array of shape\n",
    "`(nTime, M)` where `M = model.M`, and the latter will have shape `(N, nTime, M)` where\n",
    "`N` is the size of the ensemble. However, in other implementations, different choices\n",
    "of data structure may be more convenient, e.g. where the different components of the\n",
    "unknowns are merely concatenated along the last axis, rather than being kept in\n",
    "separate dicts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555112b",
   "metadata": {},
   "source": [
    "#### Permeability sampling\n",
    "We will estimate the log permeability field.  We parameterize the permeability\n",
    "parameters via some transform, which becomes part of the forward model. We term the\n",
    "parameterized permeability fields \"pre-permeability\". *If* we use the exponential,\n",
    "then we will we working with log-permeabilities. At any rate, the transform should be\n",
    "chosen so that the parameterized permeabilities are suited for ensemble methods, i.e.\n",
    "are distributed as a Gaussian.  But this consideration must be weighted against the\n",
    "fact that that nonlinearity (which is also a difficulty for ensemble methods) in the\n",
    "transform might add to the nonlinearity of the total/composite forward model.  In any\n",
    "case, since this is a synthetic case, we can freely choose *both* the distribution of\n",
    "the parameterized permeabilities, *and* the transform.  Here we use Gaussian fields,\n",
    "and a \"perturbed\" exponential function (to render the problem a little more complex)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f6ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prior_perm(N=1):\n",
    "    lperms = geostat.gaussian_fields(model.mesh(), N, r=0.8)\n",
    "    return lperms\n",
    "\n",
    "# Also configure plot parameters suitable for pre-perm\n",
    "plots.styles[\"pperm\"][\"levels\"] = np.linspace(-4, 4, 21)\n",
    "plots.styles[\"pperm\"][\"ticks\"] = np.arange(-4, 4+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3dc094",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perm_transf(x):\n",
    "    return .1 + np.exp(5*x)\n",
    "    # return 1000*np.exp(3*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1486212",
   "metadata": {},
   "source": [
    "For any type of parameter, one typically has to write a \"setter\" function that takes\n",
    "the vector of parameter parameter values, and applies it to the specific model\n",
    "implementation. We could merge this functionality with `perm_transf` (and indeed the\n",
    "\"setter\" function is also part of the composite forward model) but it is convenient to\n",
    "separate these implementation specifics from the mathematics going on in\n",
    "`perm_transf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31b375",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def set_perm(model, log_perm_array):\n",
    "    \"\"\"Set perm. in model code. Duplicates the perm. values in x- and y- dir.\"\"\"\n",
    "    p = perm_transf(log_perm_array)\n",
    "    p = p.reshape(model.shape)\n",
    "    model.Gridded.K = np.stack([p, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92a786",
   "metadata": {},
   "source": [
    "Now we are in position to sample the permeability of the (synthetic) truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm.Truth = sample_prior_perm()\n",
    "set_perm(model, perm.Truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115f07a",
   "metadata": {},
   "source": [
    "#### Wells\n",
    "In this model, wells are represented simply by point **sources** and **sinks**. This\n",
    "is of course incredibly basic and not realistic, but works for our purposes. So all we\n",
    "need to specify is their placement and flux (which we will not vary in time). The code\n",
    "below puts wells on a grid. Try `print(grid2)` to see how to easily specify another\n",
    "well configuration.\n",
    "\n",
    "Since the **boundary conditions** are Dirichlet, specifying *zero flux*, and the fluid\n",
    "is incompressible, the total of the source terms must equal that of the sinks. This is\n",
    "ensured by the `config_wells` function used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8cfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1 = [.1, .9]\n",
    "grid2 = np.dstack(np.meshgrid(grid1, grid1)).reshape((-1, 2))\n",
    "rates = np.ones((len(grid2), 1))  # ==> all wells use the same (constant) rate\n",
    "model.config_wells(\n",
    "    # Each row in `inj` and `prod` should be a tuple: (x, y, rate),\n",
    "    # where x, y ∈ (0, 1) and rate > 0.\n",
    "    inj  = [[0.50, 0.50, 1.00]],\n",
    "    prod = np.hstack((grid2, rates)),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b541a",
   "metadata": {},
   "source": [
    "#### Plot\n",
    "Let's take a moment to visualize the (true) model permeability field, and the well locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01f7c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"True perm. field\", figsize=(1.5, 1), rel=1)\n",
    "# plots.field(ax, perm.Truth, \"pperm\")\n",
    "plots.field(ax, perm_transf(perm.Truth),\n",
    "            locator=LogLocator(), wells=True, colorbar=True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c4ef89",
   "metadata": {},
   "source": [
    "#### Observation operator\n",
    "The data will consist in the water saturation of at the well locations, i.e. of the\n",
    "production. I.e. there is no well model. It should be pointed out, however, that\n",
    "ensemble methods technically support observation models of any complexity, though your\n",
    "accuracy mileage may vary (again, depending on the incurred nonlinearity and\n",
    "non-Gaussianity). Furthermore, it is also no problem to include time-dependence in the\n",
    "observation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22b562",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "obs_inds = [model.xy2ind(x, y) for (x, y, _) in model.producers]\n",
    "def obs_model(water_sat):\n",
    "    return water_sat[obs_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d0e19",
   "metadata": {},
   "source": [
    "#### Simulation\n",
    "The following generates the synthetic truth evolution and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16872a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1\n",
    "dt = 0.025\n",
    "nTime = round(T/dt)\n",
    "wsat.init.Truth = np.zeros(model.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a07392",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.past.Truth,\n",
    " prod.past.Truth) = misc.repeat(model.step, nTime, wsat.init.Truth, dt, obs_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b5add8",
   "metadata": {},
   "source": [
    "#### Animation\n",
    "Run the code cells below to get an animation of the oil saturation evolution.\n",
    "Injection/production wells are marked with triangles pointing down/up.\n",
    "The (untransformed) pre-perm field is plotted, rather than the actual permeability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f833e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "animation = plots.dashboard(\"Truth\", perm, wsat.past, prod.past);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: can take up to a minute to appear\n",
    "animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9a851",
   "metadata": {},
   "source": [
    "#### Noisy obs\n",
    "In reality, observations are never perfect. To emulate this, we corrupt the\n",
    "observations by adding a bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac156c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "prod.past.Noisy = prod.past.Truth.copy()\n",
    "nProd           = len(model.producers)  # num. of obs (each time)\n",
    "R               = 1e-3 * np.eye(nProd)\n",
    "for iT in range(nTime):\n",
    "    prod.past.Noisy[iT] += sqrt(R) @ rnd.randn(nProd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad0c37a",
   "metadata": {},
   "source": [
    "Plot of observations (and their noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"Observations\")\n",
    "plots.production1(ax, prod.past.Truth, prod.past.Noisy);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec91e58",
   "metadata": {},
   "source": [
    "## Prior\n",
    "The prior ensemble is generated in the same manner as the (synthetic) truth, using the\n",
    "same mean and covariance.  Thus, the members are \"statistically indistinguishable\" to\n",
    "the truth. This assumption underlies ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "perm.Prior = sample_prior_perm(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0969395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that field (before transformation) is Gaussian with (expected) mean 0 and variance 1.\n",
    "print(\"Prior mean:\", np.mean(perm.Prior))\n",
    "print(\"Prior var.:\", np.var(perm.Prior))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebcaa1",
   "metadata": {},
   "source": [
    "#### Histogram\n",
    "Let us inspect the parameter values in the form of their histogram.\n",
    "Note that the histogram of the truth is simply counting the values of a single field,\n",
    "whereas the histogram of the ensemble counts the values of `N` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c1494",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"Perm.\", figsize=(1.5, .7), rel=1)\n",
    "bins = np.linspace(*plots.styles[\"pperm\"][\"levels\"][[0, -1]], 32)\n",
    "for label, perm_field in perm.items():\n",
    "    x = perm_field.ravel()\n",
    "    ax.hist(perm_transf(x),\n",
    "            perm_transf(bins),\n",
    "            # Divide counts by N to emulate `density=1` for log-scale.\n",
    "            weights=(np.ones_like(x)/N if label != \"Truth\" else None),\n",
    "            label=label, alpha=0.3)\n",
    "ax.set(xscale=\"log\", xlabel=\"Permeability\", ylabel=\"Count\")\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce481243",
   "metadata": {},
   "source": [
    "Since the x-scale is logarithmic, the prior's histogram should look Gaussian if\n",
    "`perm_transf` is purely exponential. By contrast, the historgram of the truth is from\n",
    "a single (spatially extensive) realisation, and therefore will contain significant\n",
    "sampling \"error\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4440d26",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Below we can see some (pre-perm) realizations (members) from the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.fields(perm.Prior, \"pperm\", \"Prior\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa02f9",
   "metadata": {},
   "source": [
    "#### Variance/Spectrum\n",
    "In practice, of course, we would not be using an explicit `Cov` matrix when generating\n",
    "the prior ensemble, because it would be too large.  However, since this synthetic case\n",
    "in being made that way, let's inspect its spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c24ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, svals, VT = sla.svd(perm.Prior)\n",
    "plots.spectrum(svals, \"Prior cov.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabb697",
   "metadata": {},
   "source": [
    "With our limited ensemble size, we see no clear cutoff index. In other words, we are\n",
    "not so fortunate that the prior is implicitly restricted to some subspace that is of\n",
    "lower rank than our ensemble. This is a very realistic situation, and indicates that\n",
    "localisation (implemented further below) will be very beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8309b2c",
   "metadata": {},
   "source": [
    "## Forward model (ensemble propagation)\n",
    "Ensemble methods obtain observation-parameter sensitivities from the\n",
    "covariances of the ensemble run through the (\"forward\") model.  This is a\n",
    "composite function.  The main work consists of running the reservoir\n",
    "simulator for each realisation in the ensemble.  However, the simulator only\n",
    "inputs/outputs state variables, so we also have to take the necessary steps\n",
    "to set the parameter values. Finally it all has to be stitched together;\n",
    "this is not usually a pleasant task, giving rise to tools like\n",
    "[ERT](https://github.com/equinor/ert)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050f6b58",
   "metadata": {},
   "source": [
    "A huge technical advantage of ensembel methods is that they are\n",
    "\"embarrasingly parallelizable\", because each iterate is complete independent\n",
    "(requires no communication) from the others.\n",
    "We take advantage of this through multiprocessing which, in Python,\n",
    "requires very little code overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bc135",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set (int) number of CPU cores to use. Set to False when debugging.\n",
    "multiprocess = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39727cb1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def forward_model(nTime, *args, desc=\"\"):\n",
    "    \"\"\"Create the (composite) forward model, i.e. forecast. Supports ensemble input.\"\"\"\n",
    "\n",
    "    def run1(estimable):\n",
    "        \"\"\"Forward model for a *single* member/realisation.\"\"\"\n",
    "        # Avoid the risk (difficult to diagnose with multiprocessing) that\n",
    "        # the parameter values of one member overwrite those of another.\n",
    "        # Alternative: re-initialize the model.\n",
    "        model_n = copy.deepcopy(model)\n",
    "\n",
    "        # Unpack variables\n",
    "        wsat0, perm, *rates = estimable\n",
    "\n",
    "        # Set production rates, if provided.\n",
    "        if rates:\n",
    "            # The historical rates (couble be, but) are not unknowns;\n",
    "            # Instead, this \"setter\" is provided for the purpose\n",
    "            # of optimising future production.\n",
    "            model_n.producers[:, 2] = rates[0]\n",
    "            model_n.config_wells(model_n.injectors, model_n.producers, remap=False)\n",
    "\n",
    "        # Set permeabilities\n",
    "        set_perm(model_n, perm)\n",
    "\n",
    "        # Run simulator\n",
    "        wsats, prods = misc.repeat(\n",
    "            model_n.step, nTime, wsat0, dt, obs_model, pbar=False)\n",
    "\n",
    "        return wsats, prods\n",
    "\n",
    "    # Compose ensemble. This packing is a technicality necessary for\n",
    "    # the syntax of `map`, used instead of a `for`-loop for multiprocessing.\n",
    "    E = zip(*args)  # Tranpose args (so that member_index is 0th axis)\n",
    "\n",
    "    # Dispatch jobs\n",
    "    desc = \" \".join([\"Ens.simul.\", desc])\n",
    "    if multiprocess:\n",
    "        from p_tqdm import p_map\n",
    "        n = None if isinstance(multiprocess, bool) else multiprocess\n",
    "        Ef = list(p_map(run1, list(E), num_cpus=n, desc=desc))\n",
    "    else:\n",
    "        Ef = list(progbar(map(run1, E), desc, N))\n",
    "\n",
    "    # Transpose (to unpack)\n",
    "    # Here we output everything, but really we need only emit\n",
    "    # - The state at the final time, for restarts (predictions).\n",
    "    # - The observations (for the assimilation update).\n",
    "    # - The variables used for production optimisation\n",
    "    #   (in this case the same as the obs, namely the production).\n",
    "    saturation, production = zip(*Ef)\n",
    "\n",
    "    return np.array(saturation), np.array(production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da018fd",
   "metadata": {},
   "source": [
    "Note that the forward model not only takes an ensemble of permeability fields, but\n",
    "also an ensemble of initial water saturations. This is not because the initial\n",
    "saturations are uncertain (unknown); indeed, this here case study assumes that it is\n",
    "perfectly known, and equal to the true initial water saturation (a constant field of\n",
    "0). Therefore, the initial water saturation is set to the true value for each member\n",
    "(giving it uncertainty 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec28cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsat.init.Prior = np.tile(wsat.init.Truth, (N, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef173e62",
   "metadata": {},
   "source": [
    "So why does `forward_model` have saturation as an input and output? Because the\n",
    "posterior of this state (i.e. time-dependent, prognostic) variable *does* depend on\n",
    "the method used for the conditioning, and will later be used to restart the\n",
    "simulations so as to generate future predictions.\n",
    "\n",
    "Now, let's run the forward model on the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.past.Prior,\n",
    " prod.past.Prior) = forward_model(nTime, wsat.init.Prior, perm.Prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cf0f4",
   "metadata": {},
   "source": [
    "## Localisation (*optional*)\n",
    "If you choose not to run this section, then you must use a fairly large ensemble size\n",
    "in order to obtain results of any value.\n",
    "\n",
    "Localisation invervenes to fix-up the estimated correlations before they are used. It\n",
    "is a method of injecting prior information (distant points are likely not strongly\n",
    "codependent) that is not *encoded* in the ensemble (usually due to their finite size).\n",
    "Defining an effective localisation mask or tapering function can be a difficult task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3121ac",
   "metadata": {},
   "source": [
    "### Correlation plots\n",
    "The conditioning \"update\" of ensemble methods is often formulated in terms of a\n",
    "\"**Kalman gain**\" matrix, derived so as to achieve a variety of optimality properties\n",
    "(see e.g. [[Jaz70]](#Jaz70)): in the linear-Gaussian case, to compute the correct\n",
    "posterior moments; in the linear (non-Gaussian) case, to compute the BLUE, or achieve\n",
    "orthogonality of the posterior error and innovation; in the non-linear, non-Gaussian\n",
    "case, the ensemble version can be derived as linear regression (with some tweaks) from\n",
    "the perturbed observations to the unknowns.\n",
    "\n",
    "Another way to look at it is to ask \"what does it do?\". Heuristically, this may be\n",
    "answered in 3 points:\n",
    "\n",
    "- It uses *estimated correlation* coefficients to establish relationships between\n",
    "  observations and unknowns. For example, if there is no correlation, there will\n",
    "  be no update (even for iterative methods).\n",
    "- It takes into account the variables' scales *and* relative uncertainties, via their\n",
    "  variances. Hence why it works with covariances, and not just correlations. One of\n",
    "  the main advantages of ensemble methods is that the estimation inherently provides\n",
    "  reduced-rank representations of covariance matrices.\n",
    "- It takes into account the \"intermingling\" of correlations. For example, two\n",
    "  measurements/observations that are highly correlated (when including both prior and\n",
    "  observation errors) will barely contribute more than either one.\n",
    "\n",
    "In summary, it is useful to investigate the correlation relations of the ensemble,\n",
    "especially for the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8796a5",
   "metadata": {},
   "source": [
    "#### Auto-correlation for `wsat`\n",
    "First, as a sanity check, it is useful to plot the correlation of the saturation field\n",
    "at some given time vs. the production at the same time. The correlation should be\n",
    "maximal (1.00) at the location of the well in question. Let us verify this: zoom-in\n",
    "several times (not available on Colab), centering on the green star, to verify that it\n",
    "lies on top of the well of that panel.  The green stars mark the location of the\n",
    "maximum of the correlation field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = wsat.past.Prior[:, -1]\n",
    "bb = prod.past.Prior[:, -1].T\n",
    "corrs = [misc.corr(A, b) for b in bb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d146a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plots.fields(corrs, \"corr\", \"Saturation vs. obs\", argmax=True, wells=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d504a5",
   "metadata": {},
   "source": [
    "#### Interactive correlation plot\n",
    "The following plots a variety of different correlation fields. Each field may\n",
    "be seen as a single column (or row) of a larger (\"cross\")-covariance matrix,\n",
    "which would typically be too large for explicit computation or storage. The\n",
    "following solution, though, which computes the correlation fields \"on the\n",
    "fly\", should be viable for relatively large scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_comp(Field, T, Point, t, x, y):\n",
    "    A = prior_fields[Field]\n",
    "    b = prior_fields[Point]\n",
    "    if A.ndim > 2: A = A[:, T]  # noqa\n",
    "    if b.ndim > 2: b = b[:, t]  # noqa\n",
    "    b = b[:, model.sub2ind(x, y)]\n",
    "    return misc.corr(A, b)\n",
    "\n",
    "prior_fields = {\n",
    "    \"Saturation\": wsat.past.Prior,\n",
    "    \"Pre-perm\": perm.Prior,\n",
    "}\n",
    "\n",
    "corr_comp.controls = dict(\n",
    "    Field = list(prior_fields),\n",
    "    T = (0, nTime),\n",
    "    Point = list(prior_fields),\n",
    "    t = (0, nTime),\n",
    "    x = (0, model.Nx-1),\n",
    "    y = (0, model.Ny-1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827491bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plots.field_interact(corr_comp, \"corr\", \"Field(T) vs. Point(t, x, y)\", argmax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80d4c3",
   "metadata": {},
   "source": [
    "Use the interative control widgets to investigate the correlation structure.\n",
    "Answer and discuss the following questions:\n",
    "\n",
    "- For each combination of `Field` and `Point`:\n",
    "  - Set `T` or `t` to 0. How do the correlation fields look? Why?\n",
    "  - Set `T` or `t` to 1. How do the correlation fields look? Why?\n",
    "- Set `T = t = 20` and `Field = Point = Saturation`. Why is the green marker\n",
    "  (showing the location of the maximum) on top of the crosshairs?\n",
    "  Does this hold when `Field != Point` (hint: try moving `x` and `y`)?\n",
    "- Set both `Field` and `Point` to `Pre-perm`, and put the point somewhere near the center.\n",
    "  Why is the correlation field so regular (almost perfectly circular or elliptic)?\n",
    "- TODO: Add more remarks/questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72bf05",
   "metadata": {},
   "source": [
    "### Tapering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f02697",
   "metadata": {},
   "source": [
    "#### Plot of localized domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036929ea",
   "metadata": {},
   "source": [
    "#### Plot of localized correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89b0b2",
   "metadata": {},
   "source": [
    "## Assimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ad082",
   "metadata": {},
   "source": [
    "### Ensemble smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24394c94",
   "metadata": {},
   "source": [
    "#### Why smoothing?\n",
    "Why do we only use smoothers (and not filters) for history matching?  When ensemble\n",
    "methods were first being used for history matching, it was though that filtering,\n",
    "rather than smoothing, should be used.  Filters sequentially assimilate the\n",
    "time-series data, running the model simulator in between each observation time,\n",
    "(re)starting each step from saturation fields that have been conditioned on all of the\n",
    "data up until that point.  Typically, the filters would be augmented with parameter\n",
    "fields (time-independent unknowns) as well. Either way, re-starting the simulator with\n",
    "ensemble-updated fields tends to be problematic, because the updated members might not\n",
    "be physically realistic and realisable, causing the simulator's solver to slow down or\n",
    "fail to converge. This issue is generally aggravated by not having run the simulator\n",
    "from time 0, since the linear updates provided by the ensemble will yield saturation\n",
    "fields that differ from those obtained by re-running the simulator. Therefore,\n",
    "updating the unknowns only once, using all of the observations, is far more\n",
    "convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f8d1d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ES_update:\n",
    "    \"\"\"Update/conditioning (Bayes' rule) of an ensemble, given a vector of obs.\n",
    "\n",
    "    Implements the \"ensemble smoother\" (ES) algorithm,\n",
    "    with \"perturbed observations\".\n",
    "    NB: obs_err_cov is treated as diagonal. Alternative: use `sla.sqrtm`.\n",
    "\n",
    "    Why have we chosen to use a class (and not a function)?\n",
    "    Because this allows storing `KGdY`, for later use.\n",
    "    This \"on-the-fly\" application follows directly from state-augmentation formalism.\n",
    "\n",
    "    NB: some of these formulae appear transposed, and reversed,\n",
    "    compared to (EnKF) literature standards. The reason is that\n",
    "    we stack the members as rows instead of the conventional columns.\n",
    "    Rationale: https://nansencenter.github.io/DAPPER/dapper/index.html#conventions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_ens, observations, obs_err_cov):\n",
    "        \"\"\"Prepare the update.\"\"\"\n",
    "        Y, _        = center(obs_ens, rescale=True)\n",
    "        obs_cov     = obs_err_cov*(N-1) + Y.T@Y\n",
    "        obs_pert    = rnd.randn(N, len(observations)) @ sqrt(obs_err_cov)\n",
    "        innovations = observations - (obs_ens + obs_pert)\n",
    "\n",
    "        # (pre-) Kalman gain * Innovations\n",
    "        # Also called the X5 matrix by Evensen'2003.\n",
    "        self.KGdY = innovations @ sla.pinv2(obs_cov) @ Y.T\n",
    "\n",
    "    def __call__(self, E):\n",
    "        \"\"\"Do the update.\"\"\"\n",
    "        return E + self.KGdY @ center(E)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaef507",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c75cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ravel_time(x, undo=False):\n",
    "    \"\"\"Ravel/flatten the last two axes, or undo this operation.\"\"\"\n",
    "    if undo:\n",
    "        *N, ab = x.shape\n",
    "        return x.reshape(N + [nTime, ab//nTime])\n",
    "    else:\n",
    "        *N, a, b = x.shape\n",
    "        return x.reshape(N + [a*b])\n",
    "\n",
    "# Pre-compute\n",
    "ES = ES_update(\n",
    "    obs_ens      = ravel_time(prod.past.Prior),\n",
    "    observations = ravel_time(prod.past.Noisy),\n",
    "    obs_err_cov  = sla.block_diag(*[R]*nTime),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply\n",
    "perm.ES = ES(perm.Prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276134",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Let's plot the updated, initial ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e25718",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "plots.fields(perm.ES, \"pperm\", \"ES (posterior)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73f69c",
   "metadata": {},
   "source": [
    "We will see some more diagnostics later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb643fec",
   "metadata": {},
   "source": [
    "### Iterative ensemble smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63b75b",
   "metadata": {},
   "source": [
    "#### Why iterate?\n",
    "Because of the non-linearity of the forward model, the likelihood does not\n",
    "have a Gaussian shape, and the ensemble methods will not compute the true\n",
    "posterior (even with infinite `N`).  However, after performing an update, it\n",
    "may be expected that the estimate of the sensitivity (of the model to the\n",
    "observations) has improved. Therefore it makes sense to retry the update\n",
    "(starting from the prior again, so as not to over-condition/use the data),\n",
    "but this time with the improved sensitivity estimate.\n",
    "This cycle can then be repeated indefinitely.\n",
    "\n",
    "However, the meaning of \"improvement\" of the sensitivity estimate is not well defined.\n",
    "It is known that ensemble sensitivities estimate the *average* sensitivities\n",
    "([[Raa19]](#Raa19)); however, it is not trivial to argue that the average\n",
    "defined by the (iteratively approximated) posterior is better suited than\n",
    "that of the prior, as neither one will yield the correct posterior.\n",
    "Nevertheless, when accompanied by sufficient hand-waving, most people will\n",
    "feel convinced by the above argument, or something similar.\n",
    "\n",
    "Another perspective is that the iterations *might* manage to find the\n",
    "mode of the posterior, i.e. perform maximum-a-posteriori (MAP) estimation.\n",
    "This perspective comes from weather forecasting and their \"variational\"\n",
    "methods, as well as classical (extended, iterative) Kalman filtering.\n",
    "However, this perspective is more of a first-order approximation\n",
    "to the fully Bayesian uncertainty quantification approximated by ensemble methods.\n",
    "\n",
    "In any case, empricial evidence leave little room to doubt that iterations\n",
    "yield improved estiamtion accuracy, albeit a the cost of (linearly) more\n",
    "computational effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f08f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IES_analysis(w, T, Y, dy):\n",
    "    \"\"\"Compute the ensemble analysis.\"\"\"\n",
    "    N = len(Y)\n",
    "    Y0       = sla.pinv(T) @ Y        # \"De-condition\"\n",
    "    V, s, UT = misc.svd0(Y0)          # Decompose\n",
    "    Cowp     = misc.pows(V, misc.pad0(s**2, N) + N-1)\n",
    "    Cow1     = Cowp(-1.0)             # Posterior cov of w\n",
    "    grad     = Y0@dy - w*(N-1)        # Cost function gradient\n",
    "    dw       = grad@Cow1              # Gauss-Newton step\n",
    "    T        = Cowp(-.5) * sqrt(N-1)  # Transform matrix\n",
    "    return dw, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09afec28",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def IES(ensemble, observations, obs_err_cov, stepsize=1, nIter=10, wtol=1e-4):\n",
    "    \"\"\"Iterative ensemble smoother.\"\"\"\n",
    "    E = ensemble\n",
    "    y = observations\n",
    "    N = len(E)\n",
    "    N1 = N - 1\n",
    "    Rm12T = np.diag(sqrt(1/np.diag(obs_err_cov)))  # TODO?\n",
    "\n",
    "    # Init\n",
    "    stat = Dict(dw=[], rmse=[], stepsize=[],\n",
    "                obj=Dict(lklhd=[], prior=[], postr=[]))\n",
    "\n",
    "    # Init ensemble decomposition.\n",
    "    X0, x0 = center(E)    # Decompose ensemble.\n",
    "    w      = np.zeros(N)  # Control vector for the mean state.\n",
    "    T      = np.eye(N)    # Anomalies transform matrix.\n",
    "\n",
    "    for itr in range(nIter):\n",
    "        # Compute rmse (vs. Truth)\n",
    "        stat.rmse += [misc.RMSM(E, perm.Truth).rmse]\n",
    "\n",
    "        # Forecast.\n",
    "        _, Eo = forward_model(nTime, wsat.init.Prior, E, desc=f\"Iter #{itr}\")\n",
    "        Eo = ravel_time(Eo)\n",
    "\n",
    "        # Prepare analysis.\n",
    "        Y, xo  = center(Eo)         # Get anomalies, mean.\n",
    "        dy     = (y - xo) @ Rm12T   # Transform obs space.\n",
    "        Y      = Y        @ Rm12T   # Transform obs space.\n",
    "\n",
    "        # Diagnostics\n",
    "        stat.obj.prior += [w@w * N1]\n",
    "        stat.obj.lklhd += [dy@dy]\n",
    "        stat.obj.postr += [stat.obj.prior[-1] + stat.obj.lklhd[-1]]\n",
    "\n",
    "        reject_step = itr > 0 and stat.obj.postr[itr] > np.min(stat.obj.postr)\n",
    "        if reject_step:\n",
    "            # Restore prev. ensemble, lower stepsize\n",
    "            stepsize   /= 10\n",
    "            w, T        = old  # noqa\n",
    "        else:\n",
    "            # Store current ensemble, boost stepsize\n",
    "            old         = w, T\n",
    "            stepsize   *= 2\n",
    "            stepsize    = min(1, stepsize)\n",
    "\n",
    "            dw, T = IES_analysis(w, T, Y, dy)\n",
    "\n",
    "        stat.dw += [dw@dw / N]\n",
    "        stat.stepsize += [stepsize]\n",
    "\n",
    "        # Step\n",
    "        w = w + stepsize*dw\n",
    "        E = x0 + (w + T)@X0\n",
    "\n",
    "        if stepsize * np.sqrt(dw@dw/N) < wtol:\n",
    "            break\n",
    "\n",
    "    # The last step must be discarded,\n",
    "    # because it cannot be validated without re-running the model.\n",
    "    w, T = old\n",
    "    E = x0 + (w+T)@X0\n",
    "\n",
    "    return E, stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febefcb8",
   "metadata": {},
   "source": [
    "#### Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm.IES, stats_IES = IES(\n",
    "    ensemble     = perm.Prior,\n",
    "    observations = ravel_time(prod.past.Noisy),\n",
    "    obs_err_cov  = sla.block_diag(*[R]*nTime),\n",
    "    stepsize=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32827a9",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Let's plot the updated, initial ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.fields(perm.IES, \"pperm\", \"IES (posterior)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd91c7",
   "metadata": {},
   "source": [
    "The following plots the cost function(s) together with the error compared to the true\n",
    "(pre-)perm field as a function of the iteration number. Note that the relationship\n",
    "between the (total, i.e. posterior) cost function  and the RMSE is not necessarily\n",
    "monotonic. Re-running the experiments with a different seed is instructive. It may be\n",
    "observed that the iterations are not always very successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a58da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = freshfig(\"IES Objective function\")\n",
    "ls = dict(postr=\"-\", prior=\":\", lklhd=\"--\")\n",
    "for name, J in stats_IES.obj.items():\n",
    "    ax.plot(np.sqrt(J), color=\"b\", ls=ls[name], label=name)\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"RMS mismatch\", color=\"b\")\n",
    "ax.tick_params(axis='y', labelcolor=\"b\")\n",
    "ax.legend()\n",
    "ax2 = ax.twinx()  # axis for rmse\n",
    "ax2.set_ylabel('RMS error', color=\"r\")\n",
    "ax2.plot(stats_IES.rmse, color=\"r\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b12ef4",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "In terms of root-mean-square error (RMSE), the ES is expected to improve on the prior.\n",
    "The \"expectation\" wording indicates that this is true on average, but not always. To\n",
    "be specific, it means that it is guaranteed to hold true if the RMSE is calculated for\n",
    "infinitely many experiments (each time simulating a new synthetic truth and\n",
    "observations from the prior). The reason for this is that the ES uses the Kalman\n",
    "update, which is the BLUE (best linear unbiased estimate), and \"best\" means that the\n",
    "variance must get reduced. However, note that this requires the ensemble to be\n",
    "infinitely big, which it most certainly is not in our case. Therefore, we do not need\n",
    "to be very unlucky to observe that the RMSE has actually increased. Despite this, as\n",
    "we will see later, the data match might yield a different conclusions concerning the\n",
    "utility of the update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1f3d8",
   "metadata": {},
   "source": [
    "### Means vs. True field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97040b4",
   "metadata": {},
   "source": [
    "#### RMS summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats vs. true field\\n\")\n",
    "misc.RMSMs(perm, vs=\"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada437f5",
   "metadata": {},
   "source": [
    "#### Field plots\n",
    "Let's plot mean fields.\n",
    "\n",
    "NB: Caution! Mean fields are liable to smoother than the truth. This is a phenomenon\n",
    "familiar from geostatistics (e.g. Kriging). As such, their importance must not be\n",
    "overstated (they're just one estimator out of many). Instead, whenever a decision is\n",
    "to be made, all of the members should be included in the decision-making process. This\n",
    "does not mean that you must eyeball each field, but that decision analyses should be\n",
    "based on expected values with respect to ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_means = Dict({k: perm[k].mean(axis=0) for k in perm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e54053",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.fields(perm_means, \"pperm\", \"Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d542ba5",
   "metadata": {},
   "source": [
    "### Means vs. Data mismatch (past production)\n",
    "In synthetic experiments such as this one, is is instructive to computing the \"error\":\n",
    "the difference/mismatch of the (supposedly) unknown parameters and the truth.  Of\n",
    "course, in real life, the truth is not known.  Moreover, at the end of the day, we\n",
    "mainly care about production rates and saturations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03adbbc5",
   "metadata": {},
   "source": [
    "#### Re-run\n",
    "Therefore, let us now compute the \"residual\" (i.e. the mismatch between\n",
    "predicted and true *observations*), which we get from the predicted\n",
    "production \"profiles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.past.ES,\n",
    " prod.past.ES) = forward_model(nTime, wsat.init.Prior, perm.ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff13859",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.past.IES,\n",
    " prod.past.IES) = forward_model(nTime, wsat.init.Prior, perm.IES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbe564",
   "metadata": {},
   "source": [
    "It is Bayesian(ally) consistent to apply the pre-computed ES gain to any\n",
    "un-conditioned ensemble, e.g. that of the prior's production predictions. This can be\n",
    "seen (by those familiar with that trick) by state augmentation. This provides another\n",
    "posterior approximation of the production history -- one which doesn't require running\n",
    "the model again (in contrast to what we did for `prod.past.(I)ES` immediately above).\n",
    "Since it requires 0 iterations, let's call this \"ES0\". Let us try that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ad8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.past.ES0 = ravel_time(ES(ravel_time(prod.past.Prior)), undo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b438bae",
   "metadata": {},
   "source": [
    "#### Production plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.productions(prod.past, \"Past\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85969a4a",
   "metadata": {},
   "source": [
    "#### RMS summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats vs. past production (i.e. NOISY observations)\\n\")\n",
    "misc.RMSMs(prod.past, vs=\"Noisy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13e5a1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Note that the data mismatch is significantly reduced. This may be the case even if the\n",
    "updated permeability field did not have a reduced rmse (overall, relative to that of\n",
    "the prior prior). The \"direct\" forecast (essentially just linear regression) may\n",
    "achieve even lower rmse, but generally, less realistic production plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc38dbb",
   "metadata": {},
   "source": [
    "##### Comment on prior\n",
    "Note that the prior \"surrounds\" the data. This the likely situation in our synthetic\n",
    "case, where the truth was generated by the same random draw process as the ensemble.\n",
    "\n",
    "In practice, this is often not the case. If so, you might want to go back to your\n",
    "geologists and tell them that something is amiss. You should then produce a revised\n",
    "prior with better properties.\n",
    "\n",
    "Note: the above instructions sound like statistical heresy. We are using the data\n",
    "twice over (on the prior, and later to update/condition the prior). However, this is\n",
    "justified to the extent that prior information is difficult to quantify and encode.\n",
    "Too much prior adaptation, however, and you risk overfitting! Indeed, it is a delicate\n",
    "matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22fc79d",
   "metadata": {},
   "source": [
    "##### Comment on posterior\n",
    "If the assumptions (statistical indistinguishability, Gaussianity) are not too far\n",
    "off, then the ensemble posteriors (ES, EnKS, ES0) should also surround the data, but\n",
    "with a tighter fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51022f58",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "We now prediction the future by forecasting from the current (present-time) ensembles.\n",
    "\n",
    "Note that we must use the current saturation in the \"restart\" for the predictive\n",
    "simulations. Since the estimates of the current saturation depend on the assumed\n",
    "permeability field, these estimates are also \"posterior\", and depend on the\n",
    "conditioning method used. For convenience, we first extract the slice of the current\n",
    "saturation fields (which is really the only one we make use of among those of the\n",
    "past), and plot the mean fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsat.curnt = Dict({k: v[..., -1, :] for k, v in wsat.past.items()})\n",
    "wsat_means = Dict({k: np.atleast_2d(v).mean(axis=0) for k, v in wsat.curnt.items()})\n",
    "plots.fields(wsat_means, \"oil\", \"Means\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4334c30",
   "metadata": {},
   "source": [
    "#### Run\n",
    "Now we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fce795",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Future/prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.futr.Truth,\n",
    " prod.futr.Truth) = misc.repeat(model.step, nTime, wsat.curnt.Truth, dt, obs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.futr.Prior,\n",
    " prod.futr.Prior) = forward_model(nTime, wsat.curnt.Prior, perm.Prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d17f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.futr.ES,\n",
    " prod.futr.ES) = forward_model(nTime, wsat.curnt.ES, perm.ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "(wsat.futr.IES,\n",
    " prod.futr.IES) = forward_model(nTime, wsat.curnt.IES, perm.IES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f8c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.futr.ES0 = ravel_time(ES(ravel_time(prod.futr.Prior)), undo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816056a",
   "metadata": {},
   "source": [
    "#### Production plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.productions(prod.futr, \"Future\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924749d",
   "metadata": {},
   "source": [
    "#### RMS summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stats vs. (supposedly unknown) future production\\n\")\n",
    "misc.RMSMs(prod.futr, vs=\"Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43498d9",
   "metadata": {},
   "source": [
    "## Robust optimisation\n",
    "NB: This section is very unfinished, and should not be seen as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43026311",
   "metadata": {},
   "source": [
    "This section uses EnOpt to optimise the controls: the relative rates of production of\n",
    "the wells (again, for simplicity, these will be constant in time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a23967",
   "metadata": {},
   "source": [
    "Ojective function definition: total oil from production wells. This objective function\n",
    "takes an ensemble (`*E`) of unknowns (`wsat, perm`) and controls (`rates`) and outputs\n",
    "the corresponding ensemble of total oil productions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680c6a0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def total_oil(E, rates):\n",
    "    # bounded = np.all((0 < rates) & (rates < 1), axis=1)\n",
    "    wsat, prod = forward_model(nTime, *E, rates)\n",
    "    return np.sum(prod, axis=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa4157",
   "metadata": {},
   "source": [
    "Define step modifier to improve on \"vanilla\" gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957ded5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def GDM(beta1=0.9):\n",
    "    \"\"\"Gradient descent with (historical) momentum.\"\"\"\n",
    "    grad1 = 0\n",
    "\n",
    "    def set_historical(g):\n",
    "        nonlocal grad1\n",
    "        grad1 = beta1*grad1 + (1-beta1)*g\n",
    "\n",
    "    def step(g):\n",
    "        set_historical(g)\n",
    "        return grad1\n",
    "\n",
    "    return step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d677c4",
   "metadata": {},
   "source": [
    "Define EnOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158183e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def EnOpt(obj, E, ctrls, C12, stepsize=1, nIter=10):\n",
    "    N = len(E[0])\n",
    "    stepper = GDM()\n",
    "\n",
    "    # Diagnostics\n",
    "    print(\"Initial controls:\", ctrls)\n",
    "    repeated = np.tile(ctrls, (N, 1))\n",
    "    J = obj(E, repeated).mean()\n",
    "    print(\"Total oil (mean) for initial guess: %.3f\" % J)\n",
    "\n",
    "    for _itr in progbar(range(nIter), desc=\"EnOpt\"):\n",
    "        Eu = ctrls + rnd.randn(N, len(ctrls)) @ C12.T\n",
    "        Eu = Eu.clip(1e-5)\n",
    "\n",
    "        Ej = obj(E, Eu)\n",
    "        # print(\"Total oil (mean): %.3f\"%Ej.mean())\n",
    "\n",
    "        Xu = center(Eu)[0]\n",
    "        Xj = center(Ej)[0]\n",
    "\n",
    "        G  = Xj.T @ Xu / (N-1)\n",
    "\n",
    "        du = stepper(G)\n",
    "        ctrls  = ctrls + stepsize*du\n",
    "        ctrls  = ctrls.clip(1e-5)\n",
    "\n",
    "    # Diagnostics\n",
    "    print(\"Final controls:\", ctrls)\n",
    "    repeated = np.tile(ctrls, (N, 1))\n",
    "    J = obj(E, repeated).mean()\n",
    "    print(\"Total oil (mean) after optimisation: %.3f\" % J)\n",
    "\n",
    "    return ctrls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327856c",
   "metadata": {},
   "source": [
    "Run EnOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede9d2e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rnd.seed(3)\n",
    "# ctrls0  = model.producers[:, 2]\n",
    "ctrls0  = rnd.rand(nProd)\n",
    "ctrls0 /= sum(ctrls0)\n",
    "C12     = 0.03 * np.eye(nProd)\n",
    "E       = wsat.curnt.ES, perm.ES\n",
    "# E       = wsat.curnt.IES, perm.IES\n",
    "ctrls   = EnOpt(total_oil, E, ctrls0, C12, stepsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871943d",
   "metadata": {},
   "source": [
    "## Final comments\n",
    "It is instructive to run this notebook/script again, but with a different random seed.\n",
    "This will yield a different truth, and noisy production data, and so a new\n",
    "case/problem, which may be more, or less, difficult.\n",
    "\n",
    "Another alternative is to only re-run the notebook cells starting from where the prior\n",
    "was sampled. Thus, the truth and observations will not change, yet because the prior\n",
    "sample will change, the results will change. If this change is significant (which can\n",
    "only be asserted by re-running the experiments several times), then you can not have\n",
    "much confidence in your result. In order to fix this, you must increase the ensemble\n",
    "size (to reduce sampling error), or play with the tuning parameters such as the\n",
    "localisation radius (or more generally, improve your localisation implementation).\n",
    "\n",
    "Such re-running of the synthetic experiments is similar in aim to statistical\n",
    "cross-validation. Note that it may (and should) also be applied in real applications!\n",
    "Of couse, then the truth is unknown. But even though the truth is unknown, a synthetic\n",
    "truth can be sampled from the prior uncertainty. And at the very least, the history\n",
    "matching and optimisation methods should yield improved performance (reduced errors\n",
    "and increased NPV) in the synthetic case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b83104",
   "metadata": {},
   "source": [
    "## References\n",
    "<a id=\"Jaz70\">[Jaz70]</a>: Jazwinski, A. H. 1970. *Stochastic Processes and Filtering Theory*. Vol. 63. Academic Press.\n",
    "\n",
    "<a id=\"Raa19\">[Raa19]</a>: Raanes, Patrick Nima, Andreas Størksen Stordal, and Geir Evensen. 2019. “Revising the Stochastic Iterative Ensemble Smoother.” *Nonlinear Processes in Geophysics* 26 (3): 325–38.  https://doi.org/10.5194/npg-26-325-2019."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "auto:light,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
